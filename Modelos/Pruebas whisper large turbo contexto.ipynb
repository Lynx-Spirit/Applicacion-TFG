{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e344dad-b548-4c4d-af06-38befe83b89c",
   "metadata": {},
   "source": [
    "## Procesamiento inicial de los ficheros de audio\n",
    "\n",
    "En esta parte lo 煤nico que voy a hacer va a ser transformar el fichero de auido a **.mp3**. Para todo lo que es el procesamiento de los audios, se usar谩 la librer铆a [pydub](https://pydub.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12890442-40a4-49c8-9a79-de42ed6c20e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in f:\\programas\\miniconda\\envs\\tfg\\lib\\site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e96079-3c28-4f31-a1ff-fb7bd9c73276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in f:\\programas\\miniconda\\envs\\tfg\\lib\\site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114f4c9c-f552-438a-8826-bbc31fb9276c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#Checkeo que se encuentra el archivo\n",
    "os.path.exists(\"./audios/audio_prueba.ogg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff761502-c617-490c-a984-bd459239afcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='./audios/salida/audio_prueba.mp3'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "sound = AudioSegment.from_file(\"./audios/audio_prueba.ogg\") #Puedes meter aqu铆 cualquier tipo de audio\n",
    "\n",
    "sound.export(\"./audios/salida/audio_prueba.mp3\", format=\"mp3\", bitrate=\"128k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e44115-1335-456a-b6d9-3949d8982a63",
   "metadata": {},
   "source": [
    "## Uso del modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbf57d-f6dd-469b-ab66-ca844b77b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d646b7-40e9-484c-bba4-5fe73eb66db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu121\n",
      "CUDA disponible: True\n",
      "GPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af59fbdb-5216-466f-ab85-858605235b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The length of `decoder_input_ids`, including special start tokens, prompt tokens, and previous tokens, is 23,  and `max_new_tokens` is 448. Thus, the combined length of `decoder_input_ids` and `max_new_tokens` is: 471. This exceeds the `max_target_positions` of the Whisper model: 448. You should either reduce the length of your prompt, or reduce the value of `max_new_tokens`, so that their combined length is less than 448.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m forced_decoder_ids = [(i + \u001b[32m1\u001b[39m, token.item()) \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompt_ids[\u001b[32m0\u001b[39m])]\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# === 5. Generar la transcripci贸n ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m generated_ids = model.generate(\n\u001b[32m     48\u001b[39m     **inputs,\n\u001b[32m     49\u001b[39m     forced_decoder_ids=forced_decoder_ids,\n\u001b[32m     50\u001b[39m     max_new_tokens=\u001b[32m448\u001b[39m,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# === 6. Decodificar el texto generado ===\u001b[39;00m\n\u001b[32m     54\u001b[39m transcription = processor.batch_decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\Programas\\MiniConda\\envs\\tfg\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:755\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    739\u001b[39m decoder_input_ids, kwargs = \u001b[38;5;28mself\u001b[39m._prepare_decoder_input_ids(\n\u001b[32m    740\u001b[39m     cur_bsz=cur_bsz,\n\u001b[32m    741\u001b[39m     init_tokens=init_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m    751\u001b[39m     kwargs=kwargs,\n\u001b[32m    752\u001b[39m )\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# 6.4 set max new tokens or max length\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m \u001b[38;5;28mself\u001b[39m._set_max_new_tokens_and_length(\n\u001b[32m    756\u001b[39m     config=\u001b[38;5;28mself\u001b[39m.config,\n\u001b[32m    757\u001b[39m     decoder_input_ids=decoder_input_ids,\n\u001b[32m    758\u001b[39m     generation_config=generation_config,\n\u001b[32m    759\u001b[39m )\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# 6.5 Set current `begin_index` for all logit processors\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\Programas\\MiniConda\\envs\\tfg\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:1833\u001b[39m, in \u001b[36mWhisperGenerationMixin._set_max_new_tokens_and_length\u001b[39m\u001b[34m(self, config, decoder_input_ids, generation_config)\u001b[39m\n\u001b[32m   1831\u001b[39m max_new_tokens = generation_config.max_new_tokens \u001b[38;5;28;01mif\u001b[39;00m generation_config.max_new_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_new_tokens + decoder_input_ids.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.config.max_target_positions:\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1834\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe length of `decoder_input_ids`, including special start tokens, prompt tokens, and previous tokens, is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_input_ids.shape[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1835\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and `max_new_tokens` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Thus, the combined length of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1836\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`decoder_input_ids` and `max_new_tokens` is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39mdecoder_input_ids.shape[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This exceeds the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1837\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`max_target_positions` of the Whisper model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.max_target_positions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1838\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should either reduce the length of your prompt, or reduce the value of `max_new_tokens`, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1839\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mso that their combined length is less than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.max_target_positions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1840\u001b[39m     )\n\u001b[32m   1842\u001b[39m num_initial_tokens = \u001b[38;5;28mmin\u001b[39m(config.max_target_positions // \u001b[32m2\u001b[39m - \u001b[32m1\u001b[39m, decoder_input_ids.shape[-\u001b[32m1\u001b[39m] - \u001b[32m1\u001b[39m)\n\u001b[32m   1844\u001b[39m \u001b[38;5;66;03m# Make sure we don't get larger than `max_length`\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The length of `decoder_input_ids`, including special start tokens, prompt tokens, and previous tokens, is 23,  and `max_new_tokens` is 448. Thus, the combined length of `decoder_input_ids` and `max_new_tokens` is: 471. This exceeds the `max_target_positions` of the Whisper model: 448. You should either reduce the length of your prompt, or reduce the value of `max_new_tokens`, so that their combined length is less than 448."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "\n",
    "# Configuraci贸n del dispositivo y tipo de dato\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Cargar modelo y procesador\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# === 1. Cargar el archivo de audio ===\n",
    "audio_path = \"./audios/salida/audio_prueba.mp3\"\n",
    "waveform, sampling_rate = torchaudio.load(audio_path)\n",
    "\n",
    "# === 2. Convertir el audio a 16 kHz si es necesario ===\n",
    "if sampling_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    sampling_rate = 16000\n",
    "\n",
    "# === 3. Preprocesar el audio ===\n",
    "inputs = processor(\n",
    "    waveform[0],\n",
    "    sampling_rate=sampling_rate,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Mover los datos procesados al dispositivo (GPU/CPU) y tipo de dato\n",
    "inputs = {k: v.to(device=device, dtype=torch_dtype) for k, v in inputs.items()}\n",
    "\n",
    "# === 4. Crear un \"prompt\" con los nombres ===\n",
    "personajes = \"Zephir, Garl, Gorl, Iuman, Jeckiran, Tara\"\n",
    "prompt_ids = processor.tokenizer(personajes, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Convertir los IDs en formato que acepta Whisper para forzar tokens al inicio\n",
    "forced_decoder_ids = [(i + 1, token.item()) for i, token in enumerate(prompt_ids[0])]\n",
    "\n",
    "# === 5. Generar la transcripci贸n ===\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    forced_decoder_ids=forced_decoder_ids,\n",
    "    max_new_tokens=448,\n",
    ")\n",
    "\n",
    "# === 6. Decodificar el texto generado ===\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Mostrar la transcripci贸n\n",
    "print(\"\\n Transcripci贸n:\")\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117958f-8229-4ebb-8839-2d09d228d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf497cdd-203b-4dc2-ba05-94ef9e3cb3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
